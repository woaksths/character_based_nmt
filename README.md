# character_based_nmt
Neural machine translation based on character embedding using CNN

### Written part 
(a) We learned that recurrent neural architecture can operate over variable length input
(i.e., the shape of the model parameters is independent of length of the input sentences).
Is the same true of convolutional architecture? Write one sentence to explain why or why not?

(d) Transformers, a non-recurrent sequence model with a sequence of attention-based transformer blocks.
Describe 2 advantages of a Transformer encoder over the LSTM-with-attention encoder in our NMT model.
